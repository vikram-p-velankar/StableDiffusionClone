{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4884f646-d130-4151-85ee-4a35e6eb79ec",
   "metadata": {},
   "source": [
    "## GANS MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5921082d-0374-4e22-bd20-105f799628c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 169001437/169001437 [00:12<00:00, 13827339.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vikra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to C:\\Users\\vikra/.cache\\torch\\hub\\checkpoints\\inception_v3_google-0cc3c7bd.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 104M/104M [00:05<00:00, 19.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | FID: 453.40 | IS: 2.23\n",
      "Epoch 20 | FID: 270.04 | IS: 2.83\n",
      "Epoch 30 | FID: 243.72 | IS: 2.87\n",
      "Epoch 40 | FID: 183.58 | IS: 2.97\n",
      "Epoch 50 | FID: 179.24 | IS: 3.13\n",
      "Epoch 60 | FID: 169.50 | IS: 3.11\n",
      "Epoch 70 | FID: 163.14 | IS: 3.11\n",
      "Epoch 80 | FID: 154.91 | IS: 3.31\n",
      "Epoch 90 | FID: 149.52 | IS: 3.10\n",
      "Epoch 100 | FID: 151.69 | IS: 3.29\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.models import inception_v3\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(nz, 256*8*8),\n",
    "            nn.BatchNorm1d(256*8*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (256, 8, 8)),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 3, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "nz = 100\n",
    "G = Generator(nz).to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "opt_G = optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "opt_D = optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception.eval()\n",
    "\n",
    "def get_activations(images, model, batch_size=64):\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size]\n",
    "            batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            activations.append(pred.detach().cpu().numpy())\n",
    "    return np.concatenate(activations, axis=0)\n",
    "\n",
    "def calculate_fid(act1, act2):\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\n",
    "def calculate_inception_score(preds, splits=10):\n",
    "    scores = []\n",
    "    N = preds.shape[0]\n",
    "    for i in range(splits):\n",
    "        part = preds[i * N // splits: (i+1) * N // splits]\n",
    "        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "        scores.append(np.exp(np.mean(np.sum(kl, axis=1))))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "def evaluate(generator, real_images):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(5000, nz, device=device)\n",
    "        fake_images = generator(z)\n",
    "    fake_images = (fake_images + 1) / 2\n",
    "\n",
    "    real = torch.stack([real_images[i][0] for i in range(5000)])\n",
    "    real = (real + 1) / 2\n",
    "\n",
    "    act_real = get_activations(real, inception)\n",
    "    act_fake = get_activations(fake_images, inception)\n",
    "\n",
    "    fid = calculate_fid(act_real, act_fake)\n",
    "    is_mean, is_std = calculate_inception_score(F.softmax(torch.tensor(act_fake), dim=1).numpy())\n",
    "    return fid, is_mean\n",
    "\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_imgs, _) in enumerate(dataloader):\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            b_size = real_imgs.size(0)\n",
    "            z = torch.randn(b_size, nz, device=device)\n",
    "            fake_imgs = G(z)\n",
    "\n",
    "            real_labels = torch.ones(b_size, 1, device=device)\n",
    "            fake_labels = torch.zeros(b_size, 1, device=device)\n",
    "\n",
    "            D_real = D(real_imgs)\n",
    "            D_fake = D(fake_imgs.detach())\n",
    "            D_loss = loss_fn(D_real, real_labels) + loss_fn(D_fake, fake_labels)\n",
    "\n",
    "            opt_D.zero_grad()\n",
    "            D_loss.backward()\n",
    "            opt_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            z = torch.randn(b_size, nz, device=device)\n",
    "            fake_imgs = G(z)\n",
    "            D_fake = D(fake_imgs)\n",
    "            G_loss = loss_fn(D_fake, real_labels)\n",
    "\n",
    "            opt_G.zero_grad()\n",
    "            G_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            fid, iscore = evaluate(G, dataset)\n",
    "            print(f\"Epoch {epoch+1} | FID: {fid:.2f} | IS: {iscore:.2f}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(64, nz, device=device)\n",
    "                sample_imgs = G(z)\n",
    "                sample_imgs = (sample_imgs + 1) / 2\n",
    "                save_image(sample_imgs, f\"generated_images/epoch_{epoch+1}.png\", nrow=8)\n",
    "\n",
    "train(epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6381288d-c299-4ffc-9d51-b7b3d353b5ee",
   "metadata": {},
   "source": [
    "## DIFFUSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d23d51-da0a-4c4e-a2fc-312b4682cd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.9348: 100%|█████████████████████████████████████████████████████| 1563/1563 [00:13<00:00, 113.04it/s]\n",
      "Epoch 2 | Loss: 0.8551: 100%|█████████████████████████████████████████████████████| 1563/1563 [00:14<00:00, 111.31it/s]\n",
      "Epoch 3 | Loss: 0.7318: 100%|█████████████████████████████████████████████████████| 1563/1563 [00:13<00:00, 114.62it/s]\n",
      "Epoch 4 | Loss: 0.6482: 100%|█████████████████████████████████████████████████████| 1563/1563 [00:13<00:00, 112.68it/s]\n",
      "Epoch 5 | Loss: 0.5199: 100%|█████████████████████████████████████████████████████| 1563/1563 [00:13<00:00, 113.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.inception import inception_v3\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import linalg\n",
    "from PIL import Image\n",
    "from torchvision.datasets.folder import default_loader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.net(x)\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, timesteps=500):  # Reduced timesteps\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = torch.linspace(1e-4, 0.02, timesteps).to(device)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_hats = torch.cumprod(self.alphas, dim=0).to(device)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hats[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hats[t])[:, None, None, None]\n",
    "        noise = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * noise, noise\n",
    "\n",
    "    def sample(self, model, image_size=32, n=64):\n",
    "        model.eval()\n",
    "        x = torch.randn((n, 3, image_size, image_size)).to(device)\n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            t_batch = torch.full((n,), t, dtype=torch.long).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred_noise = model(x, t_batch)\n",
    "            alpha = self.alphas[t]\n",
    "            alpha_hat = self.alpha_hats[t]\n",
    "            beta = self.betas[t]\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "            else:\n",
    "                noise = 0\n",
    "            x = (1 / torch.sqrt(alpha)) * (x - ((1 - alpha) / torch.sqrt(1 - alpha_hat)) * pred_noise) + torch.sqrt(beta) * noise\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        return x\n",
    "\n",
    "def calculate_inception_score(images, splits=10):\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    inception_model.eval()\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "    preds = []\n",
    "    for i in range(0, len(images), 32):\n",
    "        batch = images[i:i+32].to(device)\n",
    "        batch = up(batch)\n",
    "        with torch.no_grad():\n",
    "            pred = inception_model(batch)\n",
    "            preds.append(F.softmax(pred, dim=1).cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    split_scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (preds.shape[0] // splits): (k+1) * (preds.shape[0] // splits), :]\n",
    "        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "        kl = np.mean(np.sum(kl, 1))\n",
    "        split_scores.append(np.exp(kl))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "\n",
    "def calculate_fid(real_images, generated_images):\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    inception_model.eval()\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "    def get_activations(images):\n",
    "        activations = []\n",
    "        for i in range(0, len(images), 32):\n",
    "            batch = images[i:i+32].to(device)\n",
    "            batch = up(batch)\n",
    "            with torch.no_grad():\n",
    "                pred = inception_model(batch)\n",
    "            activations.append(pred.cpu().numpy())\n",
    "        return np.concatenate(activations, axis=0)\n",
    "\n",
    "    act1 = get_activations(real_images)\n",
    "    act2 = get_activations(generated_images)\n",
    "    mu1, sigma1 = np.mean(act1, axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = np.mean(act2, axis=0), np.cov(act2, rowvar=False)\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "def train(model, diffusion, dataloader, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader)\n",
    "        for images, _ in pbar:\n",
    "            images = images.to(device)\n",
    "            t = torch.randint(0, diffusion.timesteps, (images.size(0),), device=device).long()\n",
    "            x_t, noise = diffusion.noise_images(images, t)\n",
    "            noise_pred = model(x_t, t)\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
    "        torch.save(model, f'diffusion/weights_{epoch}.pth')\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=4)  # Smaller batch size\n",
    "\n",
    "model = SimpleUNet().to(device)\n",
    "diffusion = Diffusion(timesteps=500)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-6)\n",
    "\n",
    "train(model, diffusion, trainloader, optimizer, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e69bf-9867-459a-bc23-1fe589d57308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.2155: 100%|█████████████████████████████████████████████████████| 1563/1563 [00:14<00:00, 111.02it/s]\n",
      "Epoch 2 | Loss: 0.1574: 100%|█████████████████████████████████████████████████████| 1563/1563 [00:13<00:00, 111.88it/s]\n",
      "Epoch 3 | Loss: 0.1787: 100%|█████████████████████████████████████████████████████| 1563/1563 [00:14<00:00, 109.87it/s]\n",
      "Epoch 4 | Loss: 0.2709: 100%|█████████████████████████████████████████████████████| 1563/1563 [00:14<00:00, 110.17it/s]\n",
      "  0%|                                                                                         | 0/1563 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train(model, diffusion, trainloader, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad1eae1-077e-4f32-8d6b-cc790b297f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception Score: 1.0000 ± 0.0000\n",
      "FID: 2448.8362\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('diffusion/weights_49.pth')\n",
    "samples = diffusion.sample(model, n=10)\n",
    "os.makedirs(\"generated\", exist_ok=True)\n",
    "for i, img in enumerate(samples):\n",
    "    save_image(img, f\"generated/{i}.png\")\n",
    "\n",
    "real_imgs = torch.stack([trainset[i][0] for i in range(10)]).to(device)\n",
    "fake_imgs = torch.stack([transforms.ToTensor()(Image.open(f\"generated/{i}.png\")) for i in range(10)]).to(device)\n",
    "\n",
    "mean_is, std_is = calculate_inception_score(fake_imgs)\n",
    "print(f\"Inception Score: {mean_is:.4f} ± {std_is:.4f}\")\n",
    "\n",
    "fid = calculate_fid(real_imgs, fake_imgs)\n",
    "print(f\"FID: {fid:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
